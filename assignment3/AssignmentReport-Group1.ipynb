{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "### Answer\n",
    "<img src=\"./images/img1.jpg\" width=\"500\" height=\"600\">\n",
    "<br>\n",
    "<br>  \n",
    "And so on. The final answer is:  \n",
    "<br>  \n",
    "<br>  \n",
    "<img src=\"./images/img2.jpg\" width=\"500\" height=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "Max pooling layers are the most effective at reducing the sensitivity to small translational variations of the input\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "A padding of 3 pixels should be used in order to maintain the input dimensions\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "You would need a kernel of size 5x5\n",
    "\n",
    "## task 1e)\n",
    "\n",
    "This would result in half the dimensions in both directions: 254x254\n",
    "\n",
    "## task 1f)\n",
    "\n",
    "This would effectively remove the boundary pixels, resulting in: 252x252\n",
    "\n",
    "## task 1g)\n",
    "\n",
    "<img src=\"./images/img3.jpg\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "\n",
    "The model produces the following result after a total of 6 epochs of training:  \n",
    "  \n",
    "<img src=\"./plots/task2_plot1.png\">\n",
    "\n",
    "The final validation accuracy was around 72%\n",
    "\n",
    "### Task 2b)\n",
    "The metrics for the best validation model parameters are the following:\n",
    "\n",
    "Train loss:  0.42587071568155355  \n",
    "Train accuracy:  0.8541073968705548  \n",
    "  \n",
    "Validation loss:  0.8239447427749633  \n",
    "Validation accuracy:  0.728  \n",
    "  \n",
    "Test loss:  0.8394349775314331  \n",
    "Test accuracy:  0.7257\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "\n",
    "We defined a model that achieved a 78.79% test accuracy using the following setup.\n",
    "\n",
    "In the dataloader, we doubled the amount of training samples, half of which had this additional transform:  \n",
    "RandomResizedCrop(size=(32, 32), scale=(0.8, 1.0), ratio=(1, 1), antialias=True)\n",
    "\n",
    "We used the following hyperparameters:  \n",
    "\n",
    "epochs = 10  \n",
    "batch_size = 32  \n",
    "learning_rate = 0.05  \n",
    "early_stop_count = 10  \n",
    "opt = \"SGD\"  \n",
    "weight_decay = 0.001  \n",
    "\n",
    "We validated 10 times per epoch\n",
    "\n",
    "\n",
    "The model had the following architecture:\n",
    "\n",
    "Layer 1:  \n",
    "Conv2d(in: 3, out: 32, kernel size: 3, stride: 1, padding: 1, activation: ReLU)  \n",
    "BatchNorm2d\n",
    "\n",
    "Layer 1 desampling:  \n",
    "Conv2d(in: 32, out: 32, kernel size: 3, stride: 2, padding: 1, activation: ReLU)  \n",
    "BatchNorm2d\n",
    "\n",
    "Layer 2:  \n",
    "Conv2d(in: 32, out: 128, kernel size: 3, stride: 1, padding: 1, activation: ReLU)  \n",
    "BatchNorm2d\n",
    "\n",
    "Layer 2 desampling:  \n",
    "Conv2d(in: 128, out: 128, kernel size: 3, stride: 2, padding: 1, activation: ReLU)  \n",
    "BatchNorm2d\n",
    "\n",
    "Layer 3:  \n",
    "Conv2d(in: 128, out: 256, kernel size: 3, stride: 1, padding: 1, activation: ReLU)  \n",
    "BatchNorm2d\n",
    "\n",
    "Layer 3 desampling:  \n",
    "Conv2d(in: 256, out: 256, kernel size: 3, stride: 2, padding: 1, activation: ReLU)  \n",
    "BatchNorm2d\n",
    "\n",
    "Flatten\n",
    "\n",
    "Layer 4:  \n",
    "Linear(in: 4096, out: 2048, activation: ReLU)  \n",
    "BatchNorm1d\n",
    "\n",
    "Layer 5:  \n",
    "Linear(in: 2048, out: 1024, activation: ReLU)  \n",
    "BatchNorm1d\n",
    "\n",
    "Layer 6:  \n",
    "Linear(in: 1024, out: 10, activation: SoftMax)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "\n",
    "Train loss:  0.1727404485391731  \n",
    "Train accuracy:  0.9421897226173542  \n",
    "Validation loss:  0.32494856346845624  \n",
    "Validation accuracy:  0.8912  \n",
    "Test loss:  0.6972488205432892  \n",
    "Test accuracy:  0.7879  \n",
    "\n",
    "<br>\n",
    "<img src=\"./plots/task3_test78p.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "\n",
    "We tried all of the recommendations listed in the assigment, and most of the worked well except for a few.\n",
    "\n",
    "We started by reducing the kernel size to 3x3. For such low resolution images (32x32) we figured that this change was obvious. We saw better results immidiately, and credit this to the kernel being able to extract smaller, more detailed features. We also increased the number of filters for the same reason, which also helped.\n",
    "\n",
    "We only used the Random Resize Crop transform and saw an increased performance because of this. We believe that by simply adding more images to the train set that are slightly manipulated enabled the model to get a better alignment to the underlying distribution and train for longer.\n",
    "\n",
    "We mostly used MaxPool2d for desampling, but changed to strided convolutions in the end. We saw a minor improvement from this. We think that having a learnable desampling is probably beneficial to this dataset since the resolution is so small. The idea is that the MaxPool removed too much information when desampling.\n",
    "\n",
    "Batch normalization was probably the most important improvement. We also changes the batch size to 32 at the same time. We credit the improvement to a more stable inference in the convolutional layers. We think that since we are not using residual connections, this gave the model more reasonable output after each layer. We used BatchNorm in both the convolutional and linear layers.\n",
    "\n",
    "The only recommendations that gave worse result after imlementation was changing the training optimizer and activation function. We tried Adam and RMSProp. Both optimizers saw a large fall in validation accuracy. We believe this had something to do with the learning rate, momentum value, and weight decay value being inappropriate for the respective optimizers. Instead of fine-tuning these optimizers, we moved back to SGD for the remainder of the task. We also tried switching to tanh and LeakyReLU, but saw no improvement from this. ReLU is probably fine as is.\n",
    "\n",
    "We added another linear layer, and saw minimal improvements.\n",
    "\n",
    "Weight decay and droput did not help too much. Not sure why, but we guess that the convolutional layers are fairly well regularized from before, and did not see much overfitting from the validation scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3d)\n",
    "\n",
    "By adding batch normalization and going from 64 to 32 in batch size, we had these results before and after:\n",
    "\n",
    "BEFORE:\n",
    "<img src=\"./plots/task3_plot_before.png\">\n",
    "\n",
    "AFTER:\n",
    "<img src=\"./plots/task3_plot_after.png\">\n",
    "\n",
    "We see that the model trained for way longer, and had a more stable peak at around 75% validation accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3e)\n",
    "\n",
    "We added more filters, fine-tuned the Adam optimizer, added dropout in the linear layers, added more weight decay, used horizontal flip as data augmentation, and got this result:\n",
    "\n",
    "Train loss:  0.2422491681104947  \n",
    "Train accuracy:  0.9197857396870555  \n",
    "Validation loss:  0.43170610122680664  \n",
    "Validation accuracy:  0.8518  \n",
    "Test loss:  0.48984660167694094  \n",
    "Test accuracy:  0.8367  \n",
    "\n",
    "<img src=\"./plots/task3_adam4_best_plot.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3f)\n",
    "\n",
    "A typical sign of overfitting is when the validation accuracy starts to drop. However, the training was stopped sufficiently early to prevent much of this.  \n",
    "\n",
    "On the other hand, we see that the train accuracy is way higher than the validation accuracy. This does reveal some overfitting, but is to be expected. What is more interesting is that the test accuracy is somewhat lower than the validation accuracy. We suspect that some of the validation data samples had their horizontaly flipped counterparts in the train dataset. This promotes overfitting, and would explain the discrepancy between the two scores.\n",
    "\n",
    "All in all, we are satisfied with the final test accuracy of 83.67%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "We followed the recommendation of using the Adam optimizer with a learning rate of 5e-4. Other hyperparameters were the following:  \n",
    "  \n",
    "max_epochs = 6  \n",
    "batch_size = 32  \n",
    "early_stop_count = 4  \n",
    "weight_decay = 0.001  \n",
    "  \n",
    "The only data augmentation used was Resize to 224x224  \n",
    "\n",
    "This resulted in a train cycle that stopped half way through epoch 4. The best model metrics were:  \n",
    "  \n",
    "Val loss: 0.30  \n",
    "Val acc: 0.900  \n",
    "  \n",
    "On the test set, it scored:  \n",
    "  \n",
    "Test loss: 0.38  \n",
    "Test acc: 0.875  \n",
    "  \n",
    "Lastly, here is a figure showing the training and validation scores during training:  \n",
    "\n",
    "<br>\n",
    "<img src=\"./plots/task4_plot.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
